library(shiny)
runExample("01_hello")
ui <- fluidPage(
# App title ----
titlePanel("Hello Shiny!"),
# Sidebar layout with input and output definitions ----
# Main panel for displaying outputs ----
mainPanel(
# Output: Histogram ----
plotOutput(outputId = "distPlot")
)
)
# 1. It is "reactive" and therefore should be automatically
#    re-executed when inputs (input$bins) change
# 2. Its output type is a plot
output$distPlot <- renderPlot({
x    <- faithful$waiting
bins <- seq(min(x), max(x), length.out = input$bins + 1)
hist(x, breaks = bins, col = "#75AADB", border = "white",
xlab = "Waiting time to next eruption (in mins)",
main = "Histogram of waiting times")
})
}
# Define server logic required to draw a histogram ----
server <- function(input, output) {
# Histogram of the Old Faithful Geyser Data ----
# with requested number of bins
# This expression that generates a histogram is wrapped in a call
# to renderPlot to indicate that:
#
# 1. It is "reactive" and therefore should be automatically
#    re-executed when inputs (input$bins) change
# 2. Its output type is a plot
output$distPlot <- renderPlot({
x    <- faithful$waiting
bins <- seq(min(x), max(x), length.out = input$bins + 1)
hist(x, breaks = bins, col = "#75AADB", border = "white",
xlab = "Waiting time to next eruption (in mins)",
main = "Histogram of waiting times")
})
}
shinyApp(Untitled1, Untitled2)
shinyApp(ui, server)
runApp()
shiny::runApp()
semeion <- read.table("C://Users//Mayur//Documents//Advance Data Science//Assignments_SDas//ADS_FINAL//Data_Semeion//semeion.txt",
header = TRUE)
head(df)
set.seed(101) # Set Seed so that same sample can be reproduced in future also
# Now Selecting 75% of data as sample from total 'n' rows of the data
sample <- sample.int(n = nrow(df), size = floor(.80*nrow(df)), replace = F)
train <- semeion[sample, ]
test  <- semeion[-sample, ]
nrow(train)
nrow(test)
library(data.table)
mydat <- fread('https://archive.ics.uci.edu/ml/machine-learning-databases/semeion/semeion.data')
head(mydat)
semeion<-mydat[,1:256]
dataVal<-mydat[,257:266]
semeion[,which(is.na(semeion))]
semeion$digit <- NA
semeion$digit[which(mydat$V257 == 1)] <- 0
semeion$digit[which(mydat$V258 == 1)] <- 1
semeion$digit[which(mydat$V259 == 1)] <- 2
semeion$digit[which(mydat$V260 == 1)] <- 3
semeion$digit[which(mydat$V261 == 1)] <- 4
semeion$digit[which(mydat$V262 == 1)] <- 5
semeion$digit[which(mydat$V263 == 1)] <- 6
semeion$digit[which(mydat$V264 == 1)] <- 7
semeion$digit[which(mydat$V265 == 1)] <- 8
semeion$digit[which(mydat$V266 == 1)] <- 9
#semeion
#dataVal
#labels(semeion)
set.seed(101) # Set Seed so that same sample can be reproduced in future also
# Now Selecting 80% of data as sample from total 'n' rows of the data
sample <- sample.int(n = nrow(semeion), size = floor(.80*nrow(semeion)), replace = F)
CNN_Ttrain <- semeion[sample, ]
CNN_Ttest  <- semeion[-sample, ]
X_Train<-CNN_Ttrain[, -257]
Y_Train<-CNN_Ttrain[, 257]
X_Test<-CNN_Ttest[, -257]
Y_Test<-CNN_Ttest[, 257]
library(reshape2)
sapply(Y_Train, function(x) sum(is.na(x)))
# reshape
head(Y_Train)
dim(Y_Train)
#dim(X_Train) <- c(nrow(X_Train),dim(X_Train)[2])
Y_Train <- data.matrix(Y_Train)
ncol(X_Train)
dim(X_Train)[2]
library(keras)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, activation = "relu", input_shape = dim(X_Train)[2]) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = "softmax")
summary(model)
model %>% compile(
loss = "sparse_categorical_crossentropy" ,      #"categorical_crossentropy"
optimizer = 'adam',   # optimizer_rmsprop(),
metrics = c("accuracy")
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 1000, batch_size = 5,
validation_split = 0.2
)
plot(history)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, activation = "relu", input_shape = dim(X_Train)[2]) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = "softmax")
summary(model)
model %>% compile(
loss = "sparse_categorical_crossentropy" ,      #"categorical_crossentropy"
optimizer = 'adam',   # optimizer_rmsprop(),
metrics = c("accuracy")
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 100,
validation_split = 0.2
)
X_Train <- data.matrix(X_Train)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 100,
validation_split = 0.2
)
plot(history)
X_Test <- data.matrix(X_Test)
Y_Test <- data.matrix(Y_Test)
model %>% evaluate(X_Test, Y_Test,verbose = 1)
predicted_Y_Test <- model %>% predict_classes(X_Test)
model %>% predict_classes(X_Test)
model %>% evaluate(X_Test, Y_Test,verbose = 1)
model %>% compile(
loss = "sparse_categorical_crossentropy" ,      #"categorical_crossentropy"
optimizer = 'adam',   # optimizer_rmsprop(),
metrics = c("accuracy"),
verbose= 2
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 100,
validation_split = 0.2
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 100,
validation_split = 0.2,
verbose= 2
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 100,
validation_split = 0.2,
verbose= 1
)
model %>% compile(
loss = "sparse_categorical_crossentropy" ,      #"categorical_crossentropy"
optimizer = 'adam',   # optimizer_rmsprop(),
metrics = c("accuracy"),
verbose= 1
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 100,
validation_split = 0.2
)
model %>% compile(
loss = "sparse_categorical_crossentropy" ,      #"categorical_crossentropy"
optimizer = 'adam',   # optimizer_rmsprop(),
metrics = c("accuracy")
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 100,
validation_split = 0.2
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 10, batch_size = 100,
validation_split = 0.2
)
model %>% evaluate(X_Test, Y_Test,verbose = 1)
predicted_Y_Test <- model %>% predict_classes(X_Test)
table(predicted_Y_Test, Y_Test)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 10, batch_size = 128,
validation_split = 0.2
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 8, batch_size = 128,
validation_split = 0.2
)
model %>% evaluate(X_Test, Y_Test,verbose = 1)
predicted_Y_Test <- model %>% predict_classes(X_Test)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 7, batch_size = 101,
validation_split = 0.2
)
model %>% evaluate(X_Test, Y_Test,verbose = 1)
predicted_Y_Test <- model %>% predict_classes(X_Test)
runApp()
runApp()
runApp()
plot(history)
plot(history)
work<-plot(history)
plot1<-plot(history)
runApp()
runApp()
runApp()
runApp()
runApp()
model %>% evaluate(X_Test, Y_Test,verbose = 1)
model %>% evaluate(X_Test, Y_Test,verbose = 1)
runApp()
runApp()
start_time <- Sys.time()
model <- keras_model_sequential()
model %>%
layer_dense(units = 1024, activation = 'relu', input_shape =  dim(X_Train)[2]) %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = "sparse_categorical_crossentropy" ,      #"categorical_crossentropy"
optimizer = 'adam',   # optimizer_rmsprop(),
metrics = c("accuracy")
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 129,
validation_split = 0.02,
verbose = 2,
shuffle="True"
)
plot(history)
model %>% evaluate(X_Test, Y_Test,verbose = 1)
predicted_Y_Test <- model %>% predict_classes(X_Test)
table(predicted_Y_Test, Y_Test)
end_time <- Sys.time()
run_time <- end_time - start_time
print("--------------------------------------------------------------------------------------")
paste("Total run time for Keras model", run_time, " seconds")
library(data.table)
mydat <- fread('https://archive.ics.uci.edu/ml/machine-learning-databases/semeion/semeion.data')
head(mydat)
semeion<-mydat[,1:256]
dataVal<-mydat[,257:266]
semeion[,which(is.na(semeion))]
# Converting flatten data to Numeric numbers
semeion$digit <- NA
semeion$digit[which(mydat$V257 == 1)] <- 0
semeion$digit[which(mydat$V258 == 1)] <- 1
semeion$digit[which(mydat$V259 == 1)] <- 2
semeion$digit[which(mydat$V260 == 1)] <- 3
semeion$digit[which(mydat$V261 == 1)] <- 4
semeion$digit[which(mydat$V262 == 1)] <- 5
semeion$digit[which(mydat$V263 == 1)] <- 6
semeion$digit[which(mydat$V264 == 1)] <- 7
semeion$digit[which(mydat$V265 == 1)] <- 8
semeion$digit[which(mydat$V266 == 1)] <- 9
#semeion
#dataVal
#labels(semeion)
set.seed(101) # Set Seed so that same sample can be reproduced in future also
# Now Selecting 80% of data as sample from total 'n' rows of the data
sample <- sample.int(n = nrow(semeion), size = floor(.80*nrow(semeion)), replace = F)
CNN_Ttrain <- semeion[sample, ]
CNN_Ttest  <- semeion[-sample, ]
X_Train<-CNN_Ttrain[, -257]
Y_Train<-CNN_Ttrain[, 257]
X_Test<-CNN_Ttest[, -257]
Y_Test<-CNN_Ttest[, 257]
library(reshape2)
sapply(Y_Train, function(x) sum(is.na(x)))
# reshape
head(Y_Train)
dim(Y_Train)
#dim(X_Train) <- c(nrow(X_Train),dim(X_Train)[2])
Y_Train <- data.matrix(Y_Train)
X_Train <- data.matrix(X_Train)
X_Test <- data.matrix(X_Test)
Y_Test <- data.matrix(Y_Test)
ncol(X_Train)
dim(X_Train)[2]
library(keras)
#calculating time to measure the model run time using KERAS
start_time <- Sys.time()
model <- keras_model_sequential()
model %>%
layer_dense(units = 1024, activation = 'relu', input_shape =  dim(X_Train)[2]) %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.1) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = "sparse_categorical_crossentropy" ,      #"categorical_crossentropy"
optimizer = 'adam',   # optimizer_rmsprop(),
metrics = c("accuracy")
)
history <- model %>% fit(
X_Train, Y_Train,
epochs = 200, batch_size = 128,
validation_split = 0.02,
verbose = 2,
shuffle="True"
)
plot(history)
model %>% evaluate(X_Test, Y_Test,verbose = 1)
predicted_Y_Test <- model %>% predict_classes(X_Test)
table(predicted_Y_Test, Y_Test)
end_time <- Sys.time()
run_time <- end_time - start_time
print("--------------------------------------------------------------------------------------")
paste("Total run time for Keras model", run_time, " seconds")
runApp()
install.packages('rsconnect')
rsconnect::setAccountInfo(name='mayurads',
token='529E8EC6233EFD24687A82879F6DB0F0',
secret='<SECRET>')
rsconnect::setAccountInfo(name='mayurads',
token='529E8EC6233EFD24687A82879F6DB0F0',
secret='hQOWWsT0ZMXcJZaVa9xRwyJLrpAVQfl3yMz6cn9j')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
library(rsconnect)
rsconnect::deployApp('C:\Users\Mayur\Documents\Advance Data Science\Assignments_SDas\ADS_FINAL\shinybasicfiles\R Shiny.Rproj')
rsconnect::deployApp('C:/Users/Mayur/Documents/Advance Data Science/Assignments_SDas/ADS_FINAL/shinybasicfiles/R Shiny.Rproj')
runApp()
