{
    "collab_server" : "",
    "contents" : "\nlibrary(data.table)\n\n\nmydat <- fread('https://archive.ics.uci.edu/ml/machine-learning-databases/semeion/semeion.data')\nhead(mydat)\n\nsemeion<-mydat[,1:256]\ndataVal<-mydat[,257:266]\n\n\n\nsemeion[,which(is.na(semeion))]\n\n# Converting flatten data to Numeric numbers\n\n\nsemeion$digit <- NA\nsemeion$digit[which(mydat$V257 == 1)] <- 0\nsemeion$digit[which(mydat$V258 == 1)] <- 1\nsemeion$digit[which(mydat$V259 == 1)] <- 2\nsemeion$digit[which(mydat$V260 == 1)] <- 3\nsemeion$digit[which(mydat$V261 == 1)] <- 4\nsemeion$digit[which(mydat$V262 == 1)] <- 5\nsemeion$digit[which(mydat$V263 == 1)] <- 6\nsemeion$digit[which(mydat$V264 == 1)] <- 7\nsemeion$digit[which(mydat$V265 == 1)] <- 8\nsemeion$digit[which(mydat$V266 == 1)] <- 9\n#semeion\n\n\n#dataVal\n\n#labels(semeion)\n\nset.seed(101) # Set Seed so that same sample can be reproduced in future also\n# Now Selecting 80% of data as sample from total 'n' rows of the data  \nsample <- sample.int(n = nrow(semeion), size = floor(.80*nrow(semeion)), replace = F)\nCNN_Ttrain <- semeion[sample, ]\nCNN_Ttest  <- semeion[-sample, ]\n\n\n\nX_Train<-CNN_Ttrain[, -257]\nY_Train<-CNN_Ttrain[, 257]\n\n\n\nX_Test<-CNN_Ttest[, -257]\nY_Test<-CNN_Ttest[, 257]\n\nlibrary(reshape2)\nsapply(Y_Train, function(x) sum(is.na(x)))\n\n# reshape\nhead(Y_Train)\ndim(Y_Train)\n#dim(X_Train) <- c(nrow(X_Train),dim(X_Train)[2])\nY_Train <- data.matrix(Y_Train)\nX_Train <- data.matrix(X_Train)\nX_Test <- data.matrix(X_Test)\nY_Test <- data.matrix(Y_Test)\n\nncol(X_Train)\ndim(X_Train)[2]\n\nlibrary(keras)\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 128, activation = \"relu\", input_shape = dim(X_Train)[2]) %>%\n  layer_dropout(rate = 0.4) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nsummary(model)\n\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'adam',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 1000, batch_size = 5, \n  validation_split = 0.2\n)\n\nplot(history)\n\n#before running the model need to convert data to matrix form for keras model\nX_Test <- data.matrix(X_Test)\nY_Test <- data.matrix(Y_Test)\n\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\n\npredicted_Y_Test <- model %>% predict_classes(X_Test)\n\ntable(predicted_Y_Test, Y_Test)\n\n#tunning by using epochs= 200 and batch_size = 30\n\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 200, batch_size = 30, \n  validation_split = 0.2\n)\n\nplot(history)\n\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\n\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 256, activation = 'relu', input_shape =  dim(X_Train)[2]) %>% \n  layer_dropout(rate = 0.2) %>% \n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_dense(units = 10, activation = 'softmax')\nsummary(model)\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'adam',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 250, batch_size = 125, \n  validation_split = 0.01,\n    verbose= 2,\n    shuffle='True'\n)\nplot(history)\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 256, activation = 'relu', input_shape =  dim(X_Train)[2]) %>% \n  layer_dropout(rate = 0.1) %>% \n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 10, activation = 'softmax')\nsummary(model)\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'adam',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 25, batch_size = 128, \n  validation_split = 0.011,\n verbose = 2,\n    shuffle=\"True\"\n)\nplot(history)\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\n#calculating time to measure the model run time using KERAS\nstart_time <- Sys.time()\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 1024, activation = 'relu', input_shape =  dim(X_Train)[2]) %>% \n  layer_dropout(rate = 0.1) %>% \n  layer_dense(units = 512, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 256, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 10, activation = 'softmax')\nsummary(model)\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'adam',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 200, batch_size = 128, \n  validation_split = 0.02,\n verbose = 2,\n    shuffle=\"True\"\n)\nplot(history)\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\nend_time <- Sys.time()\n\nrun_time <- end_time - start_time\nprint(\"--------------------------------------------------------------------------------------\")\npaste(\"Total run time for Keras model\", run_time, \" seconds\")\n\n#calculating time to measure the model run time using KERAS\nstart_time <- Sys.time()\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 1024, activation = 'relu', input_shape =  dim(X_Train)[2]) %>% \n  layer_dropout(rate = 0.1) %>% \n  layer_dense(units = 512, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 256, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 10, activation = 'softmax')\nsummary(model)\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'adam',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 200, batch_size = 129, \n  validation_split = 0.02,\n verbose = 2,\n    shuffle=\"True\"\n)\nplot(history)\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\nend_time <- Sys.time()\n\nrun_time <- end_time - start_time\nprint(\"--------------------------------------------------------------------------------------\")\npaste(\"Total run time for Keras model\", run_time, \" seconds\")\n\nlibrary('forecast')\n\n\npredicted <- data.frame(predicted_Y_Test)\nactual <- data.frame(Y_Test)\nerror <- predicted - actual\nrmse <- sqrt(mean(error^2))\nmae <- mean(abs(error))\n\nrmse\n\nls()\n\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 128, activation = \"relu\", input_shape = dim(X_Train)[2]) %>%\n  layer_dropout(rate = 0.4) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nsummary(model)\n\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'adam',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 200, batch_size = 100, \n  validation_split = 0.2\n)\n\nplot(history)\n\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\nend_time <- Sys.time()\n\nrun_time <- end_time - start_time\nprint(\"--------------------------------------------------------------------------------------\")\npaste(\"Total run time for Keras model\", run_time, \" seconds\")\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 128, activation = \"relu\", input_shape = dim(X_Train)[2]) %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nsummary(model)\n\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'adam',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 200, batch_size = 120, \n  validation_split = 0.2\n)\n\nplot(history)\n\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\nend_time <- Sys.time()\n\nrun_time <- end_time - start_time\nprint(\"--------------------------------------------------------------------------------------\")\npaste(\"Total run time for Keras model\", run_time, \" seconds\")\n\nget_config(model)\n\nget_layer(model, index = 2)\n\nmodel$layers\n\nmodel$inputs\n\nmodel$outputs\n\n# Plot the accuracy of the training data \nplot(history$metrics$acc, main=\"Model Accuracy\", xlab = \"epoch\", ylab=\"accuracy\", col=\"blue\", type=\"l\")\n\n# Plot the accuracy of the validation data\nlines(history$metrics$val_acc, col=\"green\")\n\n# Add Legend\nlegend(\"bottomright\", c(\"train\",\"test\"), col=c(\"blue\", \"green\"), lty=c(1,1))\n\n# Plot the model loss of the training data\nplot(history$metrics$loss, main=\"Model Loss\", xlab = \"epoch\", ylab=\"loss\", col=\"blue\", type=\"l\")\n\n# Plot the model loss of the test data\nlines(history$metrics$val_loss, col=\"green\")\n\n# Add legend\nlegend(\"topright\", c(\"train\",\"test\"), col=c(\"blue\", \"green\"), lty=c(1,1))\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 128, activation = \"relu\", input_shape = dim(X_Train)[2]) %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nsummary(model)\n\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'sgd',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 200, batch_size = 128, \n  validation_split = 0.2\n)\n\nplot(history)\n\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\nend_time <- Sys.time()\n\nrun_time <- end_time - start_time\nprint(\"--------------------------------------------------------------------------------------\")\npaste(\"Total run time for Keras model\", run_time, \" seconds\")\n\n# Plot the model loss\nplot(history$metrics$loss, main=\"Model Loss\", xlab = \"epoch\", ylab=\"loss\", col=\"blue\", type=\"l\")\nlines(history$metrics$val_loss, col=\"green\")\nlegend(\"topright\", c(\"train\",\"test\"), col=c(\"blue\", \"green\"), lty=c(1,1))\n\n# Plot the model accuracy\nplot(history$metrics$acc, main=\"Model Accuracy\", xlab = \"epoch\", ylab=\"accuracy\", col=\"blue\", type=\"l\")\nlines(history$metrics$val_acc, col=\"green\")\nlegend(\"bottomright\", c(\"train\",\"test\"), col=c(\"blue\", \"green\"), lty=c(1,1))\n\n#calculating time to measure the model run time using KERAS\nstart_time <- Sys.time()\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 1024, activation = 'relu', input_shape =  dim(X_Train)[2]) %>% \n  layer_dropout(rate = 0.1) %>% \n  layer_dense(units = 512, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 256, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 10, activation = 'softmax')\nsummary(model)\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\" ,      #\"categorical_crossentropy\"\n  optimizer = 'adam',   # optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\nhistory <- model %>% fit(\n  X_Train, Y_Train,\n  epochs = 23, batch_size = 128, \n  validation_split = 0.02,\n verbose = 2,\n    shuffle=\"True\"\n)\nplot(history)\nmodel %>% evaluate(X_Test, Y_Test,verbose = 1)\npredicted_Y_Test <- model %>% predict_classes(X_Test)\ntable(predicted_Y_Test, Y_Test)\n\nend_time <- Sys.time()\n\nrun_time <- end_time - start_time\nprint(\"--------------------------------------------------------------------------------------\")\npaste(\"Total run time for Keras model\", run_time, \" seconds\")\n\npaste(\"RMSE : \", sqrt(mean((data.frame(predicted_Y_Test) - data.frame(Y_Test))^2)))\n\n# Plot the model loss\nplot(history$metrics$loss, main=\"Model Loss\", xlab = \"epoch\", ylab=\"loss\", col=\"blue\", type=\"l\")\nlines(history$metrics$val_loss, col=\"green\")\nlegend(\"topright\", c(\"train\",\"test\"), col=c(\"blue\", \"green\"), lty=c(1,1))\n\n# Plot the model accuracy\nplot(history$metrics$acc, main=\"Model Accuracy\", xlab = \"epoch\", ylab=\"accuracy\", col=\"blue\", type=\"l\")\nlines(history$metrics$val_acc, col=\"green\")\nlegend(\"bottomright\", c(\"train\",\"test\"), col=c(\"blue\", \"green\"), lty=c(1,1))\n\n\n",
    "created" : 1513204201749.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "150657203",
    "id" : "BB3E2E6E",
    "lastKnownWriteTime" : 1513202914,
    "last_content_update" : 1513202914,
    "path" : "~/Advance Data Science/Assignments_SDas/ADS_FINAL/shinybasicfiles/CNN_Semeion_Keras(1).r",
    "project_path" : "CNN_Semeion_Keras(1).r",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}